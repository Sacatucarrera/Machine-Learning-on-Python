{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*Antes de comenzar estas lineas de código debe instalar las ultimas versiones de los paquetes*\n",
    "- Scikit-learn: Buscando en anaconda scikit-learn o poniendo en el terminal conda install scikit-learn o pip install scikit-learn ---> Más info en https://scikit-learn.org/stable/install.html\n",
    "- Pandas: Buscando en anaconda pandas o poniendo en el terminal conda install pandas o pip install pandas ---> Más info en https://pandas.pydata.org/docs/getting_started/index.html\n",
    "- Numpy: Buscando en anaconda numpy o poniendo en el terminal conda install numpy o pip install numpy ---> Más info en https://numpy.org/install/\n",
    "- Imbalanced-learn: Poniendo en el terminal conda install -c conda-forge imbalanced-learn o pip install -U imbalanced-learn ---> Más info en https://imbalanced-learn.readthedocs.io/en/stable/install.html\n",
    "- Pickle: Poniendo en el terminal pip install pickle ---> Más info si no te funciona en https://stackoverflow.com/questions/48477949/not-able-to-pip-install-pickle-in-python-3-6/48477988\n",
    "- seaborn: Buscando en anaconda seaborn o poniendo en el terminal pip install seaborn o conda install seaborn ---> Más info en https://seaborn.pydata.org/installing.html\n",
    "- matplotlib: Buscando en anaconda matplotlib o poniendo en el terminal pip install -U matplotlib ---> Más info en https://matplotlib.org/users/installing.html\n",
    "- Keras: Buscando e instalando en anaconda los módulos de tensorflow y keras. También se pueden instalar usando los comandos pip install tensorflow y pip install keras ---> Más infor en https://keras.io/about/\n",
    "\n",
    "Base de datos obtenida de: https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y balanceo de datos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lo primero de todo es descargarse la base de datos. Nos quedamos solo con la carpeta PetImages de todos los elementos descargados. Dentro de esta carpeta hay otras dos, Cat y Dog. Hay que eliminar el ultimo archivo de ambas carpetas (el que acaba en .db, ya que no es una imagen). Ahora leemos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lo primero de todo es eliminar las imagenes corrompidas\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "#Valor para contar las imágenes borradas\n",
    "imagenesBorradas = 0\n",
    "\n",
    "#Bucle para leer las imagenes de las carpetas\n",
    "for folder_name in (\"Cat\", \"Dog\"):\n",
    "    folder_path = os.path.join(\"PetImages/test\", folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        #Obtenemos el camino de cada imagen\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        #Y vemos si en la cabecera los bytes tienen codificado el string JFIF\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        #Si no contiene el string\n",
    "        if not is_jfif:\n",
    "            imagenesBorradas += 1 #Contamos una imagen que borramos\n",
    "            # Y borramos la imagen\n",
    "            os.remove(fpath)\n",
    "\n",
    "#Finalmente mostramos cuantas imágenes hemos borrado\n",
    "print(\"Imagenes %d borradas\" % imagenesBorradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Extracción de datos\n",
    "lote = 32 #Numero de muestras con el que se trabaja hasta que se actualizan los parametros\n",
    "data_training = tf.keras.preprocessing.image_dataset_from_directory( #Se necesita TensorFlow version 2.3.0 o superior\n",
    "    \"PetImages/train\", #Ruta de los datos\n",
    "    label_mode = 'int', #Las imágenes se etiquetan como enteros\n",
    "    color_mode = 'rgb', #Las imágenes son en color\n",
    "    batch_size=lote, #Tamaño del lote\n",
    "    validation_split=0.2, #Proporcion de muestras de validacion\n",
    "    image_size=(180, 180), #Tamaño en píxeles de la imagen\n",
    "    subset=\"training\", #Tipo de subset\n",
    "    seed=1337, #Semilla desde la que empiezo a coger muestras\n",
    ")\n",
    "\n",
    "data_validation = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages/train\",\n",
    "    label_mode = 'int', \n",
    "    color_mode = 'rgb',\n",
    "    batch_size=lote, \n",
    "    validation_split=0.2, \n",
    "    image_size=(180, 180),\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "data_test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages/test\",\n",
    "    label_mode = 'int', \n",
    "    color_mode = 'rgb',\n",
    "    batch_size=lote, \n",
    "    image_size=(180, 180),\n",
    ")\n",
    "\n",
    "#Número de lotes\n",
    "print(\n",
    "    \"Número de lotes de entrenamiento: %d\"\n",
    "    % tf.data.experimental.cardinality(data_training)\n",
    ")\n",
    "print(\n",
    "    \"Número de lotes de validación: %d\" % tf.data.experimental.cardinality(data_validation)\n",
    ")\n",
    "print(\n",
    "    \"Número de lotes de test: %d\" % tf.data.experimental.cardinality(data_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visualizamos los datos contenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el paquete para graficar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Generamos una figura de 10x10\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for imagen, etiqueta in data_training.take(1):\n",
    "    #Pintaremos las 9 primeras imágenes\n",
    "    for i in range(9):\n",
    "        #Usamos subplot para pintar\n",
    "        ax = plt.subplot(3, 3, i + 1) #3 filas, 3 columnas y la posicion especificada por i\n",
    "        plt.imshow(imagen[i].numpy().astype(\"uint8\")) #Mostramos la imagen\n",
    "        plt.title(int(etiqueta[i])) #Y su titulo es la etiqueta asignada\n",
    "        plt.axis(\"off\") #Borramos los ejes para mejorar la visualizacion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de las imágenes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Realmente no necesitamos un preprocesado de las imágenes, puesto que los valores de los píxeles son ya las características de nuestro modelo de entrada, pero enseñaremos una técnica para aumentar nuestra base de datos cuando no tenemos muchas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aumento de datos usando rotaciones \n",
    "\n",
    "#Importamos los paquetes necesarios\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Usamos una red neuronal de preprocesado para ello\n",
    "aumentoDatos = keras.Sequential( #Para elaborar una red neuronal por capas llamamos al método sequential\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"), #Capa que ejecuta un giro aleatorio sobre el eje horizontal\n",
    "        layers.experimental.preprocessing.RandomRotation(0.1), #Giro aleatorio de valor [-2pi*0,1 , 2pi*0,1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visualizamos las nuevas imágenes generadas artificialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for imagen, _ in data_training.take(1): #Cogemos la primera imagen\n",
    "    for i in range(9):\n",
    "        nuevaImagen = aumentoDatos(imagen) #Generamos una nueva imagen\n",
    "         #Y la pintamos como antes\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(nuevaImagen[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo predictivo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lo primero de todo es preparar los datos de entrada al modelo. Lo haremos mapeando los datos con la funcion lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenamiento = data_training.map(\n",
    "  lambda x, y: (aumentoDatos(x, training=True), y))\n",
    "validacion = data_validation.map(\n",
    "  lambda x, y: (aumentoDatos(x, training=True), y))\n",
    "test = data_test.map(\n",
    "  lambda x, y: (aumentoDatos(x, training=True), y))\n",
    "\n",
    "#Y hacemos un prefetching como en el modelo de texto\n",
    "entrenamiento = entrenamiento.prefetch(buffer_size=32)\n",
    "validacion = validacion.prefetch(buffer_size=32)\n",
    "test = test.prefetch(buffer_size=32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Después se declaran las metricas de evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas de evaluación\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y se construye el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el paquete de las capas neuronales\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Primero definimos las formas de entrada\n",
    "entradas = keras.Input(shape=(180,180) + (3,))\n",
    "\n",
    "#Normalizamos los pixeles de las imagenes\n",
    "x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(entradas)\n",
    "\n",
    "#Realizamos la convolución en 2D: 32 de longitud, tamaño del kernel de 3 y saltos de dos. \n",
    "#En este caso si que usamos padding, para que todos los arrays de caracteristicas tengan el mismo tamaño\n",
    "x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "#Normaliamos de nuevo el resultado\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "#Y aplicamos la funcion de activacion a las salidas. \n",
    "x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "#Repetimos los 3 pasos anteriores (Se hacen 2 convoluciones)\n",
    "x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "activacionPasada = x  # Vamos a incluir la activacion de los bloques pasados a la red neuronal para que pueda aprender mejor\n",
    "\n",
    "for size in [128, 256, 512, 728]: #Vamos incrementando el tamaño de las convoluciones\n",
    "     \n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\")(x) #Aplica 2 convoluciones 1D en lugar de una unica convolucion 2D\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # incluimos la activacion pasada tras las 2 convoluciones\n",
    "    activacion = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "        activacionPasada\n",
    "    )\n",
    "    x = layers.add([x, activacion])  # la añadimos a la capa neuronal\n",
    "    activacionPasada = x  # Y actualizamos la activacion\n",
    "\n",
    "#Realizamos la ultima convolucion de 1024\n",
    "x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "#Agrupamos los datos usando la media global\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "#Y evitamos el sobreentrenamiento\n",
    "x = layers.Dropout(0.5)(x)\n",
    "salidas = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(entradas , salidas)\n",
    "\n",
    "#keras.utils.plot_model(model, show_shapes=True)\n",
    "#Resumen del modelo elaborado\n",
    "model.summary()\n",
    "\n",
    "# Compilamos la red, siendo la funcion de coste la entropia binaria cruzada, el optimizador adam y las metricas explicadas\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(1e-3), metrics=['accuracy',f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Especifico cada cuantos grupos actualizo los parametros de la red\n",
    "iteraciones = 5\n",
    "\n",
    "# Ajustamos el modelo a los conjuntos de entrenamiento y validacion\n",
    "historia = model.fit(entrenamiento, validation_data=validacion, epochs=iteraciones)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y evaluamos el modelo con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "En este caso, como el modelo tarda mucho en entrenarse, lo almacenamos con pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos los paquetes\n",
    "import pickle\n",
    "\n",
    "#Damos el nombre al fichero\n",
    "filename = 'modeloTexto.model'\n",
    "\n",
    "#Guardamos el modelo\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y lo podríamos cargar con estas lineas de codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('modeloTexto.model','rb'))\n",
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vemos como ha sido el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la libreria para pintar \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ploteamos como es nuestra exactitud\n",
    "plt.plot(historia.history['f1_m'])\n",
    "plt.plot(historia.history['val_f1_m'])\n",
    "plt.title('Exactitud del modelo')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.xlabel('Iteracion')\n",
    "plt.legend(['Entrenamiento', 'Validacion'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Ploteamos como se minimiza la perdida de nuestro modelo\n",
    "plt.plot(historia.history['loss'])\n",
    "plt.plot(historia.history['val_loss'])\n",
    "plt.title('Función de pérdida')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.xlabel('Iteracion')\n",
    "plt.legend(['Entrenamiento', 'Validación'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sobre las guías de usuario de estos 3 paquetes puede encontrar todas sus funcionalidades. En los siguientes código usaremos más funciones de ellos:\n",
    "- Scikit-learn: https://scikit-learn.org/stable/user_guide.html\n",
    "- Pandas: https://pandas.pydata.org/docs/user_guide/index.html\n",
    "- Numpy: https://numpy.org/doc/stable/\n",
    "- Imbalanced-learn: https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html\n",
    "- Pickle: https://docs.python.org/3/library/pickle.html\n",
    "- Seaborn: https://seaborn.pydata.org/installing.html\n",
    "- Matplotlib: https://matplotlib.org/users/index.html\n",
    "- Keras: https://keras.io/guides/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
