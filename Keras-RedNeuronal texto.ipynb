{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*Antes de comenzar estas lineas de código debe instalar las ultimas versiones de los paquetes*\n",
    "- Scikit-learn: Buscando en anaconda scikit-learn o poniendo en el terminal conda install scikit-learn o pip install scikit-learn ---> Más info en https://scikit-learn.org/stable/install.html\n",
    "- Pandas: Buscando en anaconda pandas o poniendo en el terminal conda install pandas o pip install pandas ---> Más info en https://pandas.pydata.org/docs/getting_started/index.html\n",
    "- Numpy: Buscando en anaconda numpy o poniendo en el terminal conda install numpy o pip install numpy ---> Más info en https://numpy.org/install/\n",
    "- Imbalanced-learn: Poniendo en el terminal conda install -c conda-forge imbalanced-learn o pip install -U imbalanced-learn ---> Más info en https://imbalanced-learn.readthedocs.io/en/stable/install.html\n",
    "- Pickle: Poniendo en el terminal pip install pickle ---> Más info si no te funciona en https://stackoverflow.com/questions/48477949/not-able-to-pip-install-pickle-in-python-3-6/48477988\n",
    "- seaborn: Buscando en anaconda seaborn o poniendo en el terminal pip install seaborn o conda install seaborn ---> Más info en https://seaborn.pydata.org/installing.html\n",
    "- matplotlib: Buscando en anaconda matplotlib o poniendo en el terminal pip install -U matplotlib ---> Más info en https://matplotlib.org/users/installing.html\n",
    "- Keras: Buscando e instalando en anaconda los módulos de tensorflow y keras. También se pueden instalar usando los comandos pip install tensorflow y pip install keras ---> Más infor en https://keras.io/about/\n",
    "\n",
    "Base de datos obtenida de: https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y balanceo de datos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lo primero de todo es descargarse la base de datos, guardamos la carpeta en el mismo directorio que nuestro código, y borramos todos los archivos menos las subcarpetas de pos y neg de las carpetas de train y test. Ahora leemos los datos.\n",
    "\n",
    "La base de datos es de análisis de sentimientos, siendo un 1 si este es positivo y un 0 si este es negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Extracción de datos\n",
    "lote = 32 #Numero de muestras con el que se trabaja hasta que se actualizan los parametros\n",
    "data_training = tf.keras.preprocessing.text_dataset_from_directory( #Se necesita TensorFlow version 2.3.0 o superior\n",
    "    \"aclImdb/train\", #Ruta de los datos\n",
    "    batch_size=lote, #Tamaño del lote\n",
    "    validation_split=0.2, #Proporcion de muestras de validacion\n",
    "    subset=\"training\", #Tipo de subset\n",
    "    seed=1337, #Semilla desde la que empiezo a coger muestras\n",
    ")\n",
    "data_validation = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=lote,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "data_test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "   \"aclImdb/test\", batch_size=lote\n",
    ")\n",
    "\n",
    "#Número de lotes\n",
    "print(\n",
    "    \"Número de lotes de entrenamiento: %d\"\n",
    "    % tf.data.experimental.cardinality(data_training)\n",
    ")\n",
    "print(\n",
    "    \"Número de lotes de validación: %d\" % tf.data.experimental.cardinality(data_validation)\n",
    ")\n",
    "print(\n",
    "    \"Número de lotes de prueba: %d\"\n",
    "    % tf.data.experimental.cardinality(data_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visualizamos los datos contenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucle para extraer los valores\n",
    "for texto, etiqueta in data_training.take(1):\n",
    "    for i in range(5):\n",
    "        print('Texto: ',texto.numpy()[i])\n",
    "        print('Etiqueta: ',etiqueta.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Creamos una función para eliminar las / de nuestro texto, y poner todas las letras en minúsculas\n",
    "def preparacion(texto):\n",
    "    texto_minusculas = tf.strings.lower(texto)\n",
    "    sin_barras = tf.strings.regex_replace(texto_minusculas, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        sin_barras, \"[%s]\" % re.escape(string.punctuation), \"\")\n",
    "\n",
    "\n",
    "# Definimos la vectorización.\n",
    "caracteristicas = 20000 #Número máximo de tokens de la capa (con cuantas palabras trabaja)\n",
    "long_seq = 500 #Longitud máxima de cada secuencia\n",
    "\n",
    "# Definimos la capa de vectorizacion\n",
    "capa_vectorizacion = TextVectorization(\n",
    "    #Definimos los valores\n",
    "    standardize=preparacion,\n",
    "    max_tokens=caracteristicas,\n",
    "    output_mode=\"int\", #El vector que sale es de enteros\n",
    "    output_sequence_length=long_seq,\n",
    ")\n",
    "\n",
    "#Creamos el vocabulario de nuestra capa de vectorizacion\n",
    "vocabulario = data_training.map(lambda x, y: x)\n",
    "\n",
    "#Y lo adaptamos a la capa de vectorizacion\n",
    "capa_vectorizacion.adapt(vocabulario)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Elaboramos los vectores de datos que se introducirán en la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion(texto, etiqueta):\n",
    "    texto = tf.expand_dims(texto, -1)\n",
    "    return capa_vectorizacion(texto), etiqueta\n",
    "\n",
    "# Vectorizamos los datos de entrada.\n",
    "training = data_training.map(vectorizacion)\n",
    "validation = data_validation.map(vectorizacion)\n",
    "test = data_test.map(vectorizacion)\n",
    "\n",
    "#Mejoramos el rendimiento de nuestra red haciendo un prefetching sobre nuestra memoria caché\n",
    "training = training.cache().prefetch(buffer_size=10)\n",
    "validation = validation.cache().prefetch(buffer_size=10)\n",
    "test = test.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volvemos a extraer los valores con el mismo bucle\n",
    "for texto, etiqueta in training.take(1):\n",
    "    for i in range(5):\n",
    "        print('Texto: ',texto.numpy()[i])\n",
    "        print('Etiqueta: ',etiqueta.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construccion del modelo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lo primero de todo es importar las metricas que emplearemos (explicadas en videos anteriores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas (extraidas de codigo de keras)\n",
    "\n",
    "#Métricas de evaluación\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Después definimos nuestra red, usando las métricas definidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos capas de keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Definimos las entradas del modelo, que van a ser arrays de enteros\n",
    "entradas = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Despues empleamos una capa de embedding para mapear las caracteristicas extraidas\n",
    "dimension_embedding = 128\n",
    "x = layers.Embedding(caracteristicas, dimension_embedding)(entradas)\n",
    "\n",
    "#Hacemos un dropout para evitar el overfitting\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Ejecutamos 2 capas convolucionales de una dimension para extraer bien la informacion\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x) #Convolucion de filtro de 128 valores, 7 de nucleo y saltos de 3 en 3\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x) #Habilitado el padding y la funcion de activacio ReLU\n",
    "\n",
    "#Agrupamos los resultados obtenidos\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# Y los sacamos por una densa de 128 neuronas\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Finalmente obtenemos las predicciones con una capa densa de 1 neurona\n",
    "predicciones = layers.Dense(1, activation=\"sigmoid\", name=\"predicciones\")(x)\n",
    "\n",
    "model = tf.keras.Model(entradas, predicciones)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "\n",
    "#Resumen de la red\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Entrenamos la red con los datos de entrenamiento y validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste del modelo a los datos\n",
    "historia = model.fit(training, validation_data = validation,\n",
    "                    epochs=3, #Cuantas iteraciones\n",
    "                    verbose=True) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y lo evaluamos con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Podemos ver como funciona nuestra red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos que tenemos recogido en la historia del modelo\n",
    "historia.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la libreria para pintar \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ploteamos como es nuestra exactitud\n",
    "plt.plot(historia.history['accuracy'])\n",
    "plt.plot(historia.history['val_accuracy'])\n",
    "plt.title('Exactitud del modelo')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.xlabel('Iteracion')\n",
    "plt.legend(['Entrenamiento', 'Validacion'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Ploteamos como se minimiza la perdida de nuestro modelo\n",
    "plt.plot(historia.history['loss'])\n",
    "plt.plot(historia.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sobre las guías de usuario de estos 3 paquetes puede encontrar todas sus funcionalidades. En los siguientes código usaremos más funciones de ellos:\n",
    "- Scikit-learn: https://scikit-learn.org/stable/user_guide.html\n",
    "- Pandas: https://pandas.pydata.org/docs/user_guide/index.html\n",
    "- Numpy: https://numpy.org/doc/stable/\n",
    "- Imbalanced-learn: https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html\n",
    "- Pickle: https://docs.python.org/3/library/pickle.html\n",
    "- Seaborn: https://seaborn.pydata.org/installing.html\n",
    "- Matplotlib: https://matplotlib.org/users/index.html\n",
    "- Keras: https://keras.io/guides/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
