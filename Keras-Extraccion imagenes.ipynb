{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*Antes de comenzar estas lineas de código debe instalar las ultimas versiones de los paquetes*\n",
    "- Scikit-learn: Buscando en anaconda scikit-learn o poniendo en el terminal conda install scikit-learn o pip install scikit-learn ---> Más info en https://scikit-learn.org/stable/install.html\n",
    "- Pandas: Buscando en anaconda pandas o poniendo en el terminal conda install pandas o pip install pandas ---> Más info en https://pandas.pydata.org/docs/getting_started/index.html\n",
    "- Numpy: Buscando en anaconda numpy o poniendo en el terminal conda install numpy o pip install numpy ---> Más info en https://numpy.org/install/\n",
    "- Imbalanced-learn: Poniendo en el terminal conda install -c conda-forge imbalanced-learn o pip install -U imbalanced-learn ---> Más info en https://imbalanced-learn.readthedocs.io/en/stable/install.html\n",
    "- Pickle: Poniendo en el terminal pip install pickle ---> Más info si no te funciona en https://stackoverflow.com/questions/48477949/not-able-to-pip-install-pickle-in-python-3-6/48477988\n",
    "- seaborn: Buscando en anaconda seaborn o poniendo en el terminal pip install seaborn o conda install seaborn ---> Más info en https://seaborn.pydata.org/installing.html\n",
    "- matplotlib: Buscando en anaconda matplotlib o poniendo en el terminal pip install -U matplotlib ---> Más info en https://matplotlib.org/users/installing.html\n",
    "- Keras: Buscando e instalando en anaconda los módulos de tensorflow y keras. También se pueden instalar usando los comandos pip install tensorflow y pip install keras ---> Más infor en https://keras.io/about/\n",
    "\n",
    "Base de datos obtenida de: https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y balanceo de datos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lo primero de todo es descargarse la base de datos. Nos quedamos solo con la carpeta PetImages de todos los elementos descargados. Dentro de esta carpeta hay otras dos, Cat y Dog. Hay que eliminar el ultimo archivo de ambas carpetas (el que acaba en .db, ya que no es una imagen). Ahora leemos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Extracción de datos\n",
    "lote = 32 #Numero de muestras con el que se trabaja hasta que se actualizan los parametros\n",
    "data_training = tf.keras.preprocessing.image_dataset_from_directory( #Se necesita TensorFlow version 2.3.0 o superior\n",
    "    \"PetImages\", #Ruta de los datos\n",
    "    label_mode = 'int', #Las imágenes se etiquetan como enteros\n",
    "    color_mode = 'rgb', #Las imágenes son en color\n",
    "    batch_size=lote, #Tamaño del lote\n",
    "    validation_split=0.2, #Proporcion de muestras de validacion\n",
    "    image_size=(180, 180), #Tamaño en píxeles de la imagen\n",
    "    subset=\"training\", #Tipo de subset\n",
    "    seed=1337, #Semilla desde la que empiezo a coger muestras\n",
    ")\n",
    "data_validation = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    label_mode = 'int', \n",
    "    color_mode = 'rgb',\n",
    "    batch_size=lote, \n",
    "    validation_split=0.2, \n",
    "    image_size=(180, 180),\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "#Número de lotes\n",
    "print(\n",
    "    \"Número de lotes de entrenamiento: %d\"\n",
    "    % tf.data.experimental.cardinality(data_training)\n",
    ")\n",
    "print(\n",
    "    \"Número de lotes de validación: %d\" % tf.data.experimental.cardinality(data_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visualizamos los datos contenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el paquete para graficar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Generamos una figura de 10x10\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for imagen, etiqueta in data_training.take(1):\n",
    "    #Pintaremos las 9 primeras imágenes\n",
    "    for i in range(9):\n",
    "        #Usamos subplot para pintar\n",
    "        ax = plt.subplot(3, 3, i + 1) #3 filas, 3 columnas y la posicion especificada por i\n",
    "        plt.imshow(imagen[i].numpy().astype(\"uint8\")) #Mostramos la imagen\n",
    "        plt.title(int(etiqueta[i])) #Y su titulo es la etiqueta asignada\n",
    "        plt.axis(\"off\") #Borramos los ejes para mejorar la visualizacion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de las imágenes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Realmente no necesitamos un preprocesado de las imágenes, puesto que los valores de los píxeles son ya las características de nuestro modelo de entrada, pero enseñaremos una técnica para aumentar nuestra base de datos cuando no tenemos muchas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aumento de datos usando rotaciones \n",
    "\n",
    "#Importamos los paquetes necesarios\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Usamos una red neuronal de preprocesado para ello\n",
    "aumentoDatos = keras.Sequential( #Para elaborar una red neuronal por capas llamamos al método sequential\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"), #Capa que ejecuta un giro aleatorio sobre el eje horizontal\n",
    "        layers.experimental.preprocessing.RandomRotation(0.1), #Giro aleatorio de valor [-2pi*0,2 , 2pi*0,2]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visualizamos las nuevas imágenes generadas artificialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for imagen, _ in data_training.take(1): #Cogemos la primera imagen\n",
    "    for i in range(9):\n",
    "        nuevaImagen = aumentoDatos(imagen) #Generamos una nueva imagen\n",
    "        #Y la pintamos como antes\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(nuevaImagen[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevaImagen[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Las imágenes con 256 niveles de intensidad no son muy bien procesadas por las redes neuronales, por lo que lo mejor es hacer una normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero guardamos las imagenes aumentadas con la función lambda\n",
    "data_training_aumentado = data_training.map(lambda x, y: (aumentoDatos(x, training=True), y))\n",
    "\n",
    "#Después normalizamos\n",
    "data_training_aumentado = data_training_aumentado.map(lambda x, y:(x/255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la imagen normalizada\n",
    "for imagen in data_training_aumentado.take(1): \n",
    "    print(imagen[0].numpy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sobre las guías de usuario de estos 3 paquetes puede encontrar todas sus funcionalidades. En los siguientes código usaremos más funciones de ellos:\n",
    "- Scikit-learn: https://scikit-learn.org/stable/user_guide.html\n",
    "- Pandas: https://pandas.pydata.org/docs/user_guide/index.html\n",
    "- Numpy: https://numpy.org/doc/stable/\n",
    "- Imbalanced-learn: https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html\n",
    "- Pickle: https://docs.python.org/3/library/pickle.html\n",
    "- Seaborn: https://seaborn.pydata.org/installing.html\n",
    "- Matplotlib: https://matplotlib.org/users/index.html\n",
    "- Keras: https://keras.io/guides/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
